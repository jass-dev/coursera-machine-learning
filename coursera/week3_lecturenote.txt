- logistic regression의 costfunction

선형회귀의 비용함수와 꽤 비슷하다. 다만, 선형회귀의 식을 그대로 넣을경우 매우 이상한 모양이 된다.
구불구불한 convex가 되는데, 이때 수많은 local optima가 생겨버린다.
그래서 비용함수는 이렇게 그릴 것이다.
y가 1일때 -log(h(x)), y가 0일때 -log(1-h(x)).

알다시피, 로그함수는 1을 지나가는 우상향 하는 그래프이다.
따라서, -log(x)는 아래로 볼록하다.
즉, y가 1일때 cost가 0인 케이스, 즉 함수의 결과값이 1이고 실제 관측된 결과값이 1인 경우의 cost는 0이다.

- simplified cost function and gradient descent

위 cost function은 0과 1로 나뉘어져있다.
이를 하나로 합치면 다음과 같다.

-y log(h(x)) - (1-y) log(1 - h(x))

그러면, 이때 왜 우리는 이런 모양의 cost function을 사용하는가?
이는 최대우도 측정법에 따른 것이다

그러면 gradient descent를 사용하려면 어떻게 되는가?
theta = theta - alpha * d/dxh(theta)이다.
이 값을 풀면? 신기하게도 linear regression의 식과 동일한 식이 나온다.
그러면 linear regression과 logistic regression이 동일한가?
그것은 아니다. 핵심은 h(theta)에 있다.
Linear regression의 h(theta)는 theta'X였다면, 
Logistic regression의 h(theta)는 1/1+e^-theta'X 이다.

- advanced optimization

우리는 주로 gradinet descent를 사용할것이지만, 실제로 사용할 수 있는 로직들은 더 많다.
conjugate gradinet 라던가 L-BGFS같은 것들. 이것들은 alpha를 알아서 처리해줘서 빠르게 결과를 얻을 수 있지만,
다소 복잡하다. 굳이 사용하는걸 권하지 않는다.

다만, 사용하려면 라이브러리를 불러서 사용하면 된다.
실제로 octave에서는 이런 라이브러리가 있다.

자세한 것은 week3의 코드를 참조할것. 

- multicalss classification

지금까지 두개의 class분류를 사용하는 케이스에 대해 배웠다.
그렇다면 다수의 class가 있는 케이스는 어떻게 처리할 것인가?
이에 대한 아이디어로 one vs all이라는 아이디어가 있다.
3개의 class가 있다고 할 경우, 1번 클래스와 나머지 모든 클래스, 2번 클래스와 나머지 모든 클래스...등으로
전체 클래스의 갯수만큼 두개 분류 classification을 수행하는 것이다.
예를들어, 결과값의 class가 1, 2, 3, 4일경우, 총 4번의 classfication을 수행한다.
x가 주어졌을때 y = 1인 classfication, y=2인 classification, y=3, y=4 인 4개 분류를 수행한다.
이렇게해서 총 4개의 hypothesis를 얻는다.

이렇게 n개의 hypothesis가 생기면, 이 각각의 hypo에다가 x값을 집어넣고,
y값에 대해 분류한 후, 가장 확률이 높은 놈으로 값을 주면 되시겠다.
즉, max(hi(x))로 나온 값의 i를 취한다.


- overfitting problem

regression은 과적합 문제가 발생한다. 모형이 복잡해지면 값을 정확히 예측할수는 있겠지만,
과적합에 대한 우려는 상승한다.
모형과 값 사이에 괴리가 발생하는 것을 편향이라 한다. underfit, biased 등의 단어로 표현한다.
그 반대의 case가 overfit, high variance라 부른다. 
이런 문제는 다항식의 차수가 클수록, 변수의 종류가 많을수록 발생한다.

해결책은 다음과 같다. 
1. feature의 수를 줄인다.
다항식의 차수가 높은건 multivariate로 치환할 수 있으니, 차수 문제가 있더라도 어쨋든 줄일 수 있다.
overfit 문제를 해결할 수는 있지만, 변수를 버림으로 예측 정확도를 줄이는것이 단점이겠다.

2. 표준화를 한다.
모든 변수를 사용하되 각 변수가 미치는 영향도를 낮추는것이다. 모든 변수를 보존할 수는 있다.


- cost function

표준화를 적용할때 cost function은 어떻게 되는가?
기존의 cost function뒤에 이런식으로 덧붙이는 것이다. CostFunction J(theta) + theta3^2 + theta4^2
이런식이면 theta3와 theta4가 0에 근접해야 minimum해질것이다. 

그런데 보통은 어떤 theta값을 줄여야할지 모른다. 그래서 보통은, SIGMA(thataj^2)를 costfunction에 더해준다.
이 앞에 penalty를 줄 상수로 lambda를 사용한다.
만약 lambda가 지나치게 크다면, 모든 thata에 대한 penalty가 지나치게 크므로 아예 fitting이 안되버린다.
너무 작다면 overfitting을 해결할 수 없음.
적당한 lambda값이 필요하다.


- regularized linear regression, regularized logistic regression

lambda에 적용은 명확하다. 우리의 알고리즘 볼때, 변하는부분은 거의 없다.
gradinet descent를 적용할때, thetaj - alpha*(formula) 부분을 기0억할 것이다.
사실 여기서 추가되는 것은 alpha( formula - lambda / m * thetaj부분이다.
이를 다시 정리하면,
thataj(1 - alpha * lmbda / m)으로 정리되며, 뒤에 formula 부부은 그대로다.
alpha * lambda / m 값이 양수라는 점을 고려하면, 결국 theta j * 0.99 정도 값을 곱하는 것과 같은 계산이된다.
