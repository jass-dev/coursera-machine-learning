week2부터는 과제가 있으며, 이를 해결하기위해 OCTAVE의 설치가 필요하다.
막힐경우 해당 내용은 인터넷을 참고하여 더 진행할것.

- multiple features

week1까지 진행한 regression 모델은 단순 선형회귀 (univariate) 모델이였다. 이제 feature는 n개로 늘어날 것이다.
이제 행렬은 다음과 같이 생성될것이다.

[x11, x12, x13, x14, C1]
[x21, x22, x23, x24, C2]
[x31, x32, x33, x34, C3]
...

이 모델에서 X feature는 4개이며, dataset은 3개다
이런 모델을 multivariate (다변량) linear regression model이라고 한다.

- gradient descent for multiple variables

이런 multivariable에서도 gradient descent는 가능하다.
지금까지 했던걸 유사하게 하되, 다수의 theta를 벡터로 파악하는 것이다.
다수의 수치를 가진 vector에 대한 costfunction이라고 판단할때, 기존과 방법은 같다.

- gradient descent in practice

어쨋든, 다수의 feature를 사용한다고 바뀔 것은 없다.
여전히 regression이고, gradient descent를 사용할 수 있다.
다만 여기 하나의 스킬이 있는데, 다수의 variable을 사용하다보면 scale이 다를것이고, 그러다보니 gradient descent시 진동하는 현상이 발생한다.

기존의 등고선 그래프를 기억한다면, 서로 다른 feature를 가질때 이 등고선은 매우 이상하게 그려질것이라는점을 알 수 있다. feature들의 scale이 다르기 때문에 뾰족하게 나올 것이다. 이 케이스에서, gradient descent는 진동하며 내려가고 오랜 시간 후에 도달할 수 있다.
이를 해결하기위해 비슷한 값의 범위로 맞춰주는데, 전체 domain의 범위로 값을 나눠준다던가...하는 방식이다.

여기서 추천하는 방법은 mean normalization이다. 
이건 기존에 하던 표준정규분포로 정규화하는 방법과 유사하다. 각 feature Xi에 대한 avg인 mu를 구하고, dataset의 Xi값에서 mu를 뺀 후 나눠준다.
나누는 값은 stdev를 알면 좋지만, 아니면 max - min으로 전체 range로 해도 무난하다.


- gradient descent and learning rate alpha

gradient descent가 잘 작동한다면, 이 알고리즘은 descent가 반복되면서 결국 수렴할 것이다.
다만, 어떻게 작동시키느냐에 따라 그 횟수에 차이가 있을것이다. 이를 잘 맞춰주는 것이 좋은 알고리즘이다.
그 모양은 보통 우하향한다.

그런데 이 gradient descent값의 그래프가 우상향할 수 있다.
이경우 alpha가 너무 큰것이다. 그래서 muptimum을 지나쳐버리는것이고, 과하게 지나쳤을때 그 미분계수의 절대값이 더 커져버리고, 다시 반대방향으로 더 크게 움직이고....이러면 gradient descent를 했는데 오히려 값이 우상향 발산한다.

이때의 해결법은 alpha값을 낮추는것이다.
가끔 gradient descent 그래프가 물결칠수도 있는데, 이 또한 alpha값을 낮추면 해결된다.
물론, alpha가 너무 낮으면 converge point가 너무 오래걸리므로 적당히 줘야한다. 
보통 0.001, 0.01, 0.1, 1 등의 값을 준다.


- feature and polynomial regression

polynomial regression은 다항회귀이다. 사실 다항 회귀는 multivariate와 유사하다. 
다항의 특성상 x^n이 들어갈텐데, 사실 이것을 새로운 feature x2, x3 등으로 잡아주면 된다.
다만, 이렇게 진행할때 스케일링은 매우 중요하다. ^2, ^3이 될때마다 가능한 데이터 범위는 매우 커지기 때문이다.

이제 문제는 그것이다. 어떤 모델을 선택할것인가? 어떤 feature를 사용할 것인가?
이는 점차 확인할 수 있다.


- normal equation

다시 선형회귀로 돌아가보자.
선형회귀는 cost function인 J(theta)를 최소화하는 방법이었다. 그 과정에서 gradient descent를 사용한것.
이번엔 gradinet 말고 간단히 구할 수 있는 normal equation 방법을 확인해보자.
이를 사용하면 minimum값을 바로 구할 수 있다.

사실 J가 단순히 2차함수 (단순 선형회귀분석일때) 이 모양을 알 수 있어서 theta에 대한 2차식이라고 볼 수 있다면,
당연히 그 최소값을 알 수 있다.
이 2차함수를 미분해서 그 값이 0인 지점을 찾으면 되는 것.

그러면 multivariate일때는 어떻게 하는가? 마찬가지로 VECTOR에 대한 미분을 하는 것이다.
vector에서 다루는 모든 theta i 에 대해서 편미분한 값들을 전부 통합하여 0으로 놓는다면, 그값이 답이다.
matrix로 가져가면 조금 더 간단해진다.

theta = (X'X)^-1X'y

왜 그런가? 다음을 참조할것.
https://www.geeksforgeeks.org/ml-normal-equation-in-linear-regression/

구하기만 하면 답이 나온다.
그러나, (X'X)^-1을 구해야하므로, normal eqaution은 feature의 값이 늘어날수록 느려진다.X는 N by N 행렬이기때문이다.
그러다보니 이후 feature가 많아질경우, normal equation을 적용하기는 어렵다.


- normal equation and noninvertibility

위에서 normal equation을 이용해서 minimum 값을 구하는 방법을 배웠다. 
그런데, 보면 알다시피 inverse가 붙는다. 그리고, 행렬의 inverse는 때때로 없을 수 있다.

대부분의 케이스에서 X'*X는 대부분 inversible하다. 만약 inversible하지 않다하더라도,
octave에서는 이를 해결해준다. pinv와 inv method의 차이다. pinv는 psuedo inv로, 역행렬이 없을때에도 값을 구해낼 수 있다.

X'*X가 역행렬이 없을려면, 크게 두가지 케이스가 있다.
1. feature가 서로간의 관계가 있는것이다. 가령, inch와 centimeter로 표현된 길이를 feature로 쓸경우, 이 둘은 상관관계가 있기때문에 역행렬이 생기지 않는다.
2. 적은 데이터셋보다 많은 수의 feature를 사용하려고 하는 것이다. 이에 대한 방법은 있지만, 차후에 배우게 된다.